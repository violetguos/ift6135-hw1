{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Part: Neural Network Implementation & Experiments\n",
    "\n",
    "Team:\n",
    "* Jonathan Bhimani-Burrows (20178260)\n",
    "* Arlie Coles (20121051)\n",
    "* Yue (Violet) Guo (20120727)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.mnist_reader as mnist_reader\n",
    "import numpy as np\n",
    "import math\n",
    "import copy \n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = mnist_reader.load_mnist('data/mnist', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/mnist', kind='t10k')\n",
    "X_valid = X_test[0:5000]\n",
    "y_valid = y_test[0:5000]\n",
    "X_test = X_test[5000:10000]\n",
    "y_test = y_test[5000:10000]\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTarget(targetValues):\n",
    "    # Convert to one-hot encoding\n",
    "    numClasses = np.max(targetValues) + 1\n",
    "    return np.eye(numClasses)[targetValues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the targets to one hot\n",
    "y_valid = convertTarget(y_valid)\n",
    "y_test = convertTarget(y_test)\n",
    "y_train = convertTarget(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler(object):\n",
    "    '''\n",
    "    randomly sample batches without replacement.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, targets, batch_size):\n",
    "        self.num_points = data.shape[0]\n",
    "        self.features = data.shape[1]\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.num_points)\n",
    "\n",
    "    def get_batch(self, K = None):\n",
    "        '''\n",
    "        Get a random batch without replacement \n",
    "        '''\n",
    "        \n",
    "        if not K:\n",
    "            indices = np.random.choice(self.indices, self.batch_size, replace=False)\n",
    "        else:\n",
    "            indices = np.arange(K)\n",
    "        X_batch = np.take(self.data, indices, 0)\n",
    "        y_batch = self.targets[indices]\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our own activation functions\n",
    "\n",
    "def relu(pre_activation):\n",
    "    '''\n",
    "    preactivation is a vector\n",
    "    '''\n",
    "    relu_output = np.zeros(pre_activation.shape)\n",
    "    relu_flat = relu_output.flatten()\n",
    "    for i, neuron in enumerate(pre_activation.flatten()):\n",
    "        if neuron > 0:\n",
    "            relu_flat[i] = neuron\n",
    "    relu_output = relu_flat.reshape(pre_activation.shape)\n",
    "    return relu_output\n",
    "\n",
    "def softmax_single(pre_activation):\n",
    "    '''\n",
    "    Numerically stable because subtracting the max value makes bit overflow impossible,\n",
    "    we will only have non-positive values in the vector\n",
    "    '''\n",
    "    exps = np.exp(pre_activation - np.max(pre_activation))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def softmax_multiple(pre_activation):\n",
    "    '''\n",
    "    Numerically stable because subtracting the max value makes bit overflow impossible,\n",
    "    we will only have non-positive values in the vector\n",
    "    '''\n",
    "    exps = np.exp(pre_activation - np.max(pre_activation, axis = 0))\n",
    "    return exps / np.sum(exps, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_fixed = np.array([[-0.39751495,  0.00628629],\n",
    "                        [-0.0205684,   0.26683984],\n",
    "                        [0.59675625 , 0.13841242],\n",
    "                        [-0.25439437 , 0.17629986],\n",
    "                        [0.49765604 , 0.6328421 ]])\n",
    "w2_fixed = np.array( [[-0.41283729,  0.30420197, -0.26925985 , 0.1972228 , -0.23960543],\n",
    "                        [-0.07794628,  0.05534963,  0.39403587 , 0.02447081, -0.20876543],\n",
    "                        [ 0.12031748, -0.15724041,  0.07004474 , 0.01330072,  0.37325964],\n",
    "                        [-0.13685212,  0.2450681 , -0.1039663 , -0.43493921,  0.18092754],\n",
    "                        [-0.29733443,  0.39217373,  0.21700504, -0.20839457,  0.08478064],\n",
    "                        [-0.05650999, -0.21730141, -0.20041823, -0.03229149,  0.41680238]])\n",
    "w3_fixed = np.array( [[ 0.18862318, -0.26977775 , 0.10358298,  0.04400272,  0.30614878, -0.00911728],\n",
    "                        [ 0.17972289,  0.19698613,  0.22084821, -0.07767053, -0.32146514, -0.19307932]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classErr(target, predicted):\n",
    "    '''\n",
    "    not class dependent\n",
    "    target must NOT be in one hot\n",
    "    '''\n",
    "    cnt = 0\n",
    "    #print(\"in class Err\")\n",
    "    #print(\"target \\n\", target.shape[0])\n",
    "    #print(\"predicted \\n\", predicted.shape[0])\n",
    "    #print(\"target range \\n\", np.max(target))\n",
    "    #print(\"predicted range \\n\", np.max(predicted))\n",
    "    for i in range(target.shape[0]):\n",
    "        if target[i] != predicted [i]:\n",
    "            cnt +=1\n",
    "    return float(cnt) / target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(d, dh1, dh2, m):\n",
    "    W_1 = np.zeros((dh1*d))\n",
    "    W_2 = np.zeros((dh2*dh1))\n",
    "    W_3 = np.zeros((dh2*m))\n",
    "    return W_1, W_2, W_3\n",
    "\n",
    "def glorot_init(d, dh1, dh2, m):\n",
    "    dl_1 = np.sqrt((6/(d + dh1)))\n",
    "    W_1 = np.random.uniform((-1)*dh1, dh1, (dh1, d))\n",
    "\n",
    "    dl_2 = np.sqrt(6/(dh1 + dh2))\n",
    "    W_2 = np.random.uniform((-1)*dh2, dh2, (dh2, dh1))\n",
    "\n",
    "    dl_3 = np.sqrt(6/(dh2 + m))\n",
    "    W_3 = np.random.uniform((-1)*dh3, dh3, (m, dh2))\n",
    "\n",
    "    return W_1, W_2, W_3\n",
    "\n",
    "def normal_init(d, dh1, dh2, m):\n",
    "    W_1 = np.random.normal(0, 1, (dh1, d))\n",
    "    W_2 = np.random.normal(0, 1, (dh2, dh1))\n",
    "    W_3 = np.random.normal(0, 1, (m, dh2))\n",
    "    print((W_1).shape)\n",
    "    return W_1, W_2, W_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNet():\n",
    "    def __init__(self, d, hidden_dims, m, n, init_mode='uniform', eta=3e-4, regularize=None, fixed=False):\n",
    "        self.inputDim = d #inputDim\n",
    "        self.hiddenDim = hidden_dims\n",
    "        self.outputDim = m #outputDim\n",
    "        self.regularize = regularize # lambda value\n",
    "        self.learningRate = eta\n",
    "        self.numData = n\n",
    "        self.batchErrorGradients = []\n",
    "        #may use xavier init - maybe explore this later.\n",
    "        \n",
    "        # Initial weights and biases\n",
    "        if fixed:\n",
    "            self.W_1 = w1_fixed\n",
    "            self.W_2 = w2_fixed\n",
    "            self.W_3 = w3_fixed\n",
    "        elif init_mode == 'old':\n",
    "            # from last year\n",
    "            self.W_1 = np.random.uniform(-1/np.sqrt(d), 1/np.sqrt(d), \n",
    "                                         self.hiddenDim[0]*d).reshape(self.hiddenDim[0], d)\n",
    "            self.W_2 = np.random.uniform(-1/np.sqrt(self.hiddenDim[0]), 1/np.sqrt(self.hiddenDim[0]), \n",
    "                                         self.hiddenDim[1]*self.hiddenDim[0]).reshape(self.hiddenDim[1],\n",
    "                                                                                      self.hiddenDim[0]) \n",
    "            self.W_3 = np.random.uniform(-1/np.sqrt(self.hiddenDim[1]), 1/np.sqrt(self.hiddenDim[1]),\n",
    "                                         self.hiddenDim[1]*m).reshape(m, self.hiddenDim[1]) \n",
    "        \n",
    "        elif init_mode == 'normal_init':\n",
    "            self.W_1, self.W_2, self.W_3 = normal_init(self.inputDim, self.hiddenDim[0], \n",
    "                                                       self.hiddenDim[1], self.outputDim)\n",
    "        elif init_mode == 'glorot_init':\n",
    "            self.W_1, self.W_2, self.W_3 = glorot_init(self.inputDim, self.hiddenDim[0], \n",
    "                                                       self.hiddenDim[1], self.outputDim)\n",
    "        elif init_mode == 'zero_init':\n",
    "            self.W_1, self.W_2, self.W_3 = zero_init(self.inputDim, self.hiddenDim[0], \n",
    "                                                       self.hiddenDim[1], self.outputDim)\n",
    "        \n",
    "        self.b_1 = np.zeros(self.hiddenDim[0]).reshape(self.hiddenDim[0],)\n",
    "        self.b_2 = np.zeros(self.hiddenDim[1]).reshape(self.hiddenDim[1],)\n",
    "        self.b_3 = np.zeros(m).reshape(m,)\n",
    "\n",
    "\n",
    "    def fprop(self, batchData, mode='matrix'):\n",
    "        '''\n",
    "        a switch to work for both matrix and loop\n",
    "        '''\n",
    "        \n",
    "        # hidden layer 1\n",
    "        \n",
    "        if mode == 'matrix':\n",
    "            #print('self.b1', self.b_1.shape)\n",
    "            #print('self.W_1', self.W_1.shape)\n",
    "            #print('batchData.T', batchData.T.shape)\n",
    "            stack_b1 = np.array([self.b_1,] * self.numData).T\n",
    "            #print('stack_b1', stack_b1.shape)\n",
    "            self.h_a1 = np.dot(self.W_1, batchData.T) + stack_b1\n",
    "        elif mode == 'loop':\n",
    "            self.h_a1 = np.dot(self.W_1, batchData.T) + self.b_1\n",
    "            \n",
    "\n",
    "        self.h_s1 = relu(self.h_a1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # hidden layer 2\n",
    "        if mode == 'matrix':\n",
    "            stack_b2 = np.array([self.b_2,] * self.numData).T\n",
    "            self.h_a2 = np.dot(self.W_2, self.h_s1) + stack_b2\n",
    "        elif mode == 'loop':\n",
    "            self.h_a2 = np.dot(self.W_2, self.h_s1) + self.b_2\n",
    "\n",
    "        self.h_s2 = relu(self.h_a2)\n",
    "    \n",
    "        #output layer weights    \n",
    "        if mode == 'matrix':\n",
    "            stack_b3 = np.array([self.b_3,] * self.numData).T\n",
    "            self.o_a = np.dot(self.W_3, self.h_s2) + stack_b3\n",
    "        elif mode == 'loop':\n",
    "            self.o_a = np.dot(self.W_3, self.h_s2) + self.b_3           \n",
    "        \n",
    "        # softmax of weights\n",
    "        if batchData.shape[0] == 1:\n",
    "            print('using single softmax')\n",
    "            self.o_s = softmax_single(self.o_a)\n",
    "        else:\n",
    "            self.o_s = softmax_multiple(self.o_a)\n",
    "        \n",
    "        # make predication \n",
    "        if mode == 'loop':\n",
    "            self.prediction = np.argmax(self.o_s,axis = 0)\n",
    "        elif mode == 'matrix':\n",
    "            self.prediction = np.argmax(self.o_s,axis = 0)\n",
    "        \n",
    "    def errorRate(self, y, mode='matrix'):\n",
    "        '''\n",
    "        negative log\n",
    "        -logO_s(x)\n",
    "        HAD the indexing problem for matrix mode\n",
    "        '''        \n",
    "        \n",
    "        if mode == 'loop':\n",
    "            negLog = -self.o_a[np.argmax(y)] + np.log(np.sum(np.exp(self.o_a), axis=0))\n",
    "            \n",
    "        elif mode == 'matrix':\n",
    "            negLog = []\n",
    "            print(\"y.shape in error rate\" , y.shape)\n",
    "            for i in range(y.shape[1]):\n",
    "                error_at_point = -self.o_a[np.argmax(y[:,i])][i] + np.log(np.sum(np.exp(self.o_a), axis=0))[i]\n",
    "                negLog.append(error_at_point)\n",
    "            negLog = np.array(negLog)\n",
    "            negLog = np.mean(negLog)\n",
    "\n",
    "        return negLog\n",
    "          \n",
    "    def bpropLoop(self, batchData, batchTarget):\n",
    "        '''\n",
    "        dimensions: \n",
    "        o_s: m x1\n",
    "        grad_oa : m x 1\n",
    "        hs: dh x 1\n",
    "        grad_w2: m x dh\n",
    "        grad_oa: m x n\n",
    "        grad_b2: m x n\n",
    "        grad_oa: m x n\n",
    "        W(2): m x dh\n",
    "        grad_hs: dh x n\n",
    "        grad_oa: m x n\n",
    "        grad_ha: dh x n\n",
    "        x : n x d\n",
    "        grad_W1: dh x d\n",
    "        grad_ha: dh x n\n",
    "        grad_b1: dh x n\n",
    "        '''\n",
    "\n",
    "        self.grad_oa = self.o_s - batchTarget\n",
    "        # hidden layer 3\n",
    "        self.grad_W3 = np.outer(self.grad_oa, self.h_s2.T)\n",
    "        self.grad_b3 = self.grad_oa\n",
    "        self.grad_hs2 = np.dot(self.W_3.T , self.grad_oa)\n",
    "        h_a_stack2 = np.where(self.h_a2 > 0, 1, 0)\n",
    "        self.grad_ha2 = np.multiply(self.grad_hs2, h_a_stack2)   \n",
    "\n",
    "        # hidden layer 2\n",
    "        self.grad_W2 = np.outer(self.grad_ha2, self.h_s1.T)\n",
    "        self.grad_b2 = self.grad_ha2\n",
    "        self.grad_hs1 = np.dot(self.W_2.T , self.grad_ha2)\n",
    "        h_a_stack1 = np.where(self.h_a1 > 0, 1, 0)\n",
    "        self.grad_ha1 = np.multiply(self.grad_hs1, h_a_stack1)\n",
    "        \n",
    "        # hidden layer 1        \n",
    "        self.grad_W1 = np.outer(self.grad_ha1, batchData)\n",
    "        self.grad_b1 = self.grad_ha1\n",
    "        \n",
    "        \n",
    "    def bprop_matrix(self, batchData, batchTarget):\n",
    "        '''\n",
    "        backprop using matrix only\n",
    "        '''\n",
    "        \n",
    "        self.grad_oa = self.o_s - batchTarget\n",
    "        \n",
    "        self.grad_W3 = np.matmul(self.grad_oa, self.h_s2.T)/batchData.shape[0] #!\n",
    "        self.grad_b3 = np.sum(self.grad_oa, axis=1)/batchData.shape[0] #!\n",
    "        self.grad_hs2 = np.matmul(self.W_3.T , self.grad_oa)\n",
    "        self.grad_ha2 = np.multiply(self.grad_hs2, np.where(self.h_a2 > 0, 1.0, 0.0))\n",
    "\n",
    "        \n",
    "        self.grad_W2 = np.matmul(self.grad_ha2, self.h_s1.T)/batchData.shape[0] #!\n",
    "        self.grad_b2 = np.sum(self.grad_ha2, axis =1)/batchData.shape[0]\n",
    "        self.grad_hs1 = np.matmul(self.W_2.T, self.grad_ha2)\n",
    "        self.grad_ha1 = np.multiply(self.grad_hs1, np.where(self.h_a1 > 0, 1.0, 0.0))\n",
    "        \n",
    "        self.grad_W1 = np.matmul(self.grad_ha1, batchData)/batchData.shape[0] #!\n",
    "        self.grad_b1 = np.sum(self.grad_ha1, axis=1)/batchData.shape[0] #!\n",
    "        \n",
    "    def bprop(self, batchData, batchTarget, mode='matrix'):\n",
    "        '''\n",
    "        batchTarget already in one-hot format\n",
    "        \n",
    "        NOT working for a single point\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #batch target must be m by n\n",
    "        self.grad_oa = self.o_s - batchTarget\n",
    "        i = 0\n",
    "        self.grad_W2 = [np.outer(self.grad_oa[:,i], self.h_s[:,i].T) for i in range(batchData.shape[0])]\n",
    "        self.grad_b2 = self.grad_oa \n",
    "        self.grad_hs = np.dot(self.W_2.T , self.grad_oa)\n",
    "        # Check this (dim mismatch maybe)\n",
    "        h_a_stack = np.where(self.h_a > 0, 1, 0)\n",
    "        self.grad_ha = np.multiply(self.grad_hs, h_a_stack)\n",
    "        #self.grad_W1 = [np.outer(self.grad_ha[:,i], batchData[i]) for i in range(self.numData)]\n",
    "        self.grad_W1 = [np.outer(self.grad_ha[:,i], batchData[i]) for i in range(batchData.shape[0])]\n",
    "        # temporary hack for grad_W\n",
    "        self.grad_b1 = self.grad_ha\n",
    "\n",
    "        \n",
    "        if mode == 'matrix':\n",
    "            '''\n",
    "            must avg, \n",
    "            1 pt would return a list of MAT/np array, not a NP array\n",
    "            '''\n",
    "            self.grad_W2 = np.average(np.array(self.grad_W2), axis=0)\n",
    "            self.grad_b2 = np.average(np.array(self.grad_b2), axis=1)\n",
    "            \n",
    "            self.grad_W1 = np.average(np.array(self.grad_W1), axis=0)\n",
    "            self.grad_b1 = np.average(np.array(self.grad_b1), axis=1)\n",
    "\n",
    "\n",
    "        \n",
    "    def updateParams(self):\n",
    "        if self.regularize:\n",
    "            self.W_1 -= (self.regularize[0] * np.sign(self.W_1) + 2 * self.regularize[1] * self.W_1) * self.learningRate\n",
    "            self.W_2 -= (self.regularize[2] * np.sign(self.W_2) + 2 * self.regularize[3] * self.W_2) * self.learningRate\n",
    "            self.W_3 -= (self.regularize[4] * np.sign(self.W_3) + 2 * self.regularize[5] * self.W_3) * self.learningRate\n",
    "\n",
    "    \n",
    "        self.W_1 -= self.grad_W1 * self.learningRate\n",
    "        self.W_2 -= self.grad_W2 * self.learningRate\n",
    "        self.W_3 -= self.grad_W3 * self.learningRate\n",
    "\n",
    "        self.b_1 -= self.grad_b1 * self.learningRate\n",
    "        self.b_2 -= self.grad_b2 * self.learningRate\n",
    "        self.b_3 -= self.grad_b3 * self.learningRate\n",
    "\n",
    "    def calculParam(self):\n",
    "        \"\"\"\n",
    "        calculates the total amount of parameters\n",
    "        \"\"\"\n",
    "        return (self.inputDim * self.hiddenDim[0] + self.hiddenDim[1] * self.hiddenDim[0]\n",
    "                + self.outputDim * self.hiddenDim[1] + self.inputDim + self.hiddenDim[0] \n",
    "                + self.hiddenDim[1] + self.outputDim)\n",
    "    \n",
    "    \n",
    "    def gradDescentLoop(self, batchData, batchTarget, K):\n",
    "        # Call each example in the data (over the minibatches) in a loop\n",
    "        grad_W3, grad_b3, grad_W2, grad_b2, grad_W1, grad_b1 = [], [], [], [], [], []\n",
    "        predBatch = []\n",
    "        for i in range(K):\n",
    "            self.fprop(batchData[i], mode='loop') #batchTarget[:,i]\n",
    "            self.bpropLoop(batchData[i],np.array(batchTarget[:,i]))\n",
    "            predBatch.append(self.prediction)\n",
    "            grad_W3.append(self.grad_W3)\n",
    "            grad_b3.append(self.grad_b3)            \n",
    "            grad_W2.append(self.grad_W2)\n",
    "            grad_b2.append(self.grad_b2)\n",
    "            grad_W1.append(self.grad_W1)\n",
    "            grad_b1.append(self.grad_b1)\n",
    "\n",
    "        self.grad_W3 = np.mean(np.array(grad_W3), axis=0) #! array\n",
    "        self.grad_b3 = np.mean(np.array(grad_b3), axis=0) \n",
    "        self.grad_W2 = np.mean(np.array(grad_W2), axis=0) #! array\n",
    "        self.grad_b2 = np.mean(np.array(grad_b2), axis=0) \n",
    "        self.grad_W1 = np.mean(np.array(grad_W1), axis=0) #! array\n",
    "        self.grad_b1 = np.mean(np.array(grad_b1), axis=0)\n",
    "        \n",
    "        # Update params\n",
    "        #self.updateParams()\n",
    "    \n",
    "    def fpropLoop(self, batchData, K):\n",
    "        '''\n",
    "        unlike the above def gradDescentLoop(self, batchData, batchTarget, K)\n",
    "        this function only runs batchData (this is usually in test phase)\n",
    "        through the forward prop, without calculating any gradient update rule.\n",
    "        \n",
    "        Use to get predictions\n",
    "        \n",
    "        batchData: more like test/val data\n",
    "        K: ALWAYS == batchData.shape[0]\n",
    "        \n",
    "        '''\n",
    "        predBatch = []\n",
    "        for i in range(K):\n",
    "            self.fprop(batchData[i], mode='loop') #batchTarget[:,i]\n",
    "            predBatch.append(self.prediction)\n",
    "        self.predBatch = np.array(predBatch)    \n",
    "        \n",
    "    def gradDescentMat(self, batchData, batchTarget):\n",
    "        # Feed the entire data matrix in as input\n",
    "        self.fprop(batchData)\n",
    "        self.bprop_matrix(batchData, batchTarget)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6\n",
    "\n",
    "Our training function(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error(nn, epoch, train, valid, test):\n",
    "    '''\n",
    "    calculattes error in matrix mode\n",
    "    '''\n",
    "    # Train\n",
    "    nn.numData = train[0].shape[0]\n",
    "\n",
    "    nn.fprop(train[0], mode='matrix')\n",
    "    training_loss = nn.errorRate(train[1].T, mode='matrix')\n",
    "    training_err = classErr(np.argmax(train[1], axis = 1), nn.prediction)\n",
    "    \n",
    "    # Valid\n",
    "    nn.numData = valid[0].shape[0]\n",
    "    nn.fprop(valid[0], mode='matrix') \n",
    "\n",
    "    valid_loss = nn.errorRate(valid[1].T, mode='matrix')\n",
    "    valid_err = classErr(np.argmax(valid[1], axis  =1 ), nn.prediction)\n",
    "    \n",
    "    # Test\n",
    "    nn.numData = test[0].shape[0]\n",
    "\n",
    "    nn.fprop(test[0], mode='matrix') \n",
    "    test_loss = nn.errorRate(test[1].T, mode='matrix')\n",
    "    test_err = classErr(np.argmax(test[1], axis = 1), nn.prediction)\n",
    "    \n",
    "    # Write to log file\n",
    "    with open('errors.txt', 'a+') as fp:\n",
    "        line = '{},{},{},{},{},{},{}\\n'.format(epoch, training_loss, training_err, \n",
    "                                             valid_loss, valid_err, test_loss, test_err)\n",
    "        fp.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(nn, data, target,  K, num_epoch, fixed = False): \n",
    "    '''\n",
    "    train minibtaches over num_epoch epochs (in a loop)\n",
    "    also does prediction and error calcualation\n",
    "    '''\n",
    "    # Get minibatch\n",
    "    batchSampler = BatchSampler(data, target, K)\n",
    "    numBatch = data.shape[0] // K \n",
    "    print(\"num batch in train loop \", numBatch)\n",
    "    # training loop\n",
    "    for n in range(num_epoch):\n",
    "        # Do descent and update params - this is one epoch\n",
    "        for i in range(numBatch):\n",
    "            if fixed:\n",
    "                batchData, batchTarget = batchSampler.get_batch(K)\n",
    "\n",
    "            elif not fixed:\n",
    "                batchData, batchTarget = batchSampler.get_batch()\n",
    "            #difference: another loop here\n",
    "            nn.gradDescentLoop(batchData, batchTarget.T, K)\n",
    "            nn.updateParams()\n",
    "        if n % 100 == 0:\n",
    "            nn.fpropLoop(data, data.shape[0]) \n",
    "            print(\"Cross-entropy loss at the end of epoch {}: {}\".format(n, nn.errorRate(target.T, mode = 'loop')))\n",
    "            print(\"classification error at the end of epoch {}: {}\".format(n,\n",
    "                                                    classErr(np.argmax(target, axis = 1), nn.predBatch)))        \n",
    "    \n",
    "    # finalized weights, need to fprop and get the error rate \n",
    "    # a for loop inside the prop for each elem\n",
    "    nn.fpropLoop(data, data.shape[0]) \n",
    "    print(\"End of train loop process.\")\n",
    "\n",
    "\n",
    "def train_matrix(nn, data, target, K, num_epoch, fixed=False, valid=None, test=None):\n",
    "    # Get minibatch\n",
    "    batchSampler = BatchSampler(data, target, K)\n",
    "    numBatch = data.shape[0] // K \n",
    "    \n",
    "    print(\"number of batch in train matrix\", numBatch)\n",
    "    for n in range(num_epoch):\n",
    "        for i in range(numBatch):\n",
    "            # Do descent and update params - this is one epoch\n",
    "\n",
    "            if fixed:\n",
    "                batchData, batchTarget = batchSampler.get_batch(K)\n",
    "            elif not fixed:\n",
    "                batchData, batchTarget = batchSampler.get_batch()\n",
    "            nn.numData = K\n",
    "            nn.gradDescentMat(batchData, batchTarget.T)\n",
    "            \n",
    "            nn.updateParams()\n",
    "        if n % 100 == 0:\n",
    "            print(':)')\n",
    "            #nn.fprop(batchData, mode = 'matrix') \n",
    "            #pred = np.argmax(nn.o_s, axis = 0)\n",
    "            #print(\"Cross-entropy loss at the end of epoch {}: {}\".format(n, nn.errorRate(batchTarget.T, mode = 'matrix')))\n",
    "            #print(\"classification error at the end of epoch {}: {}\".format(n,\n",
    "            #                                        classErr(np.argmax(batchTarget, axis = 1), pred ))) \n",
    "        if valid:\n",
    "            nn.numData = valid[0].shape[0]\n",
    "            show_error(nn, n, [data, target], valid, test)\n",
    "    print(\"End of train matrix process.\")\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random sample NN struct but also adhere to the constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamGenerator():\n",
    "    \"\"\"\n",
    "    Generate parameters like number of hidden units, learning rate, N for finite diff, etc\n",
    "    \"\"\"\n",
    "    def __init__(self, seed):\n",
    "        # for reproducibility\n",
    "        self.seed = seed\n",
    "    \n",
    "    def countParam(self, hiddenDim):\n",
    "        inputDim = 784\n",
    "        outputDim = 10\n",
    "        return (inputDim * hiddenDim[0] + hiddenDim[1] * hiddenDim[0]\n",
    "                + outputDim * hiddenDim[1] +inputDim + hiddenDim[0] \n",
    "                + hiddenDim[1] + outputDim)\n",
    "    \n",
    "    def hiddenUnit(self):\n",
    "        \"\"\"\n",
    "        return number of hidden units in range (0.5M, 1M)\n",
    "        \"\"\"\n",
    "        \n",
    "        constraint = False\n",
    "        \n",
    "        # keep generating until something in between 0.5 to 1 M\n",
    "        while not constraint:\n",
    "\n",
    "            h1 = np.random.randint(100, 2000)\n",
    "            h2 = np.random.randint(100, 2000)    \n",
    "            total_param = self.countParam([h1, h2])\n",
    "            constraint = (0.5 * 10e5) < plot_test.calculParam() and (10e5) > plot_test.calculParam()\n",
    "        return (h1, h2)    \n",
    "\n",
    "    def learningRate(self):\n",
    "        \"\"\"\n",
    "        sample a learning rate\n",
    "        \"\"\"\n",
    "        logLearningRate = np.random.uniform(-7.5, -4.5)\n",
    "        learningRate = np.exp(logLearningRate)\n",
    "        return learningRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(512, 784)\n",
      "total number of param in plot_test 670490\n",
      "number of batch in train matrix 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikuo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in subtract\n",
      "/Users/vikuo/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/Users/vikuo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:164: RuntimeWarning: invalid value encountered in greater\n",
      "/Users/vikuo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:170: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":)\n",
      "y.shape in error rate (10, 60000)\n"
     ]
    }
   ],
   "source": [
    "# Test to see if logging/plotting works (very simple)\n",
    "train_data = X_train#[:10000]\n",
    "train_target = y_train#[:10000]\n",
    "valid_data = X_valid#[:100]\n",
    "valid_target = y_valid#[:100]\n",
    "test_data = X_test#[:100]\n",
    "test_target = y_test#[:100]\n",
    "print(train_data.shape)\n",
    "K = 200\n",
    "num_epochs = 10\n",
    "plot_test = neuralNet(train_data.shape[1], (512, 512) , 10, K, 'normal_init')\n",
    "print(\"total number of param in plot_test\", plot_test.calculParam())\n",
    "\n",
    "train_matrix(plot_test, train_data, train_target, K, num_epochs, \n",
    "             valid=[valid_data, valid_target], test=[test_data, test_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "def plot_learning_curves(log_file):\n",
    "    with open(log_file, 'r') as fp:\n",
    "        info = fp.readlines()\n",
    "        \n",
    "    epochs = np.arange(len(info))\n",
    "    \n",
    "    training_loss, valid_loss, test_loss = [], [], []\n",
    "    training_err, valid_err, test_err = [], [], []\n",
    "    \n",
    "    for line in info:\n",
    "        split_line = line.split(',')\n",
    "        training_loss.append(float(split_line[1]))\n",
    "        training_err.append(float(split_line[2]))\n",
    "        valid_loss.append(float(split_line[3]))\n",
    "        valid_err.append(float(split_line[4]))\n",
    "        test_loss.append(float(split_line[5]))\n",
    "        test_err.append(float(split_line[6]))\n",
    "    \n",
    "    # Plot\n",
    "    plt.title(\"average cross entropy loss\")\n",
    "    plt.plot(epochs, training_loss, c='blue', linestyle='solid', label = 'train loss')\n",
    "    plt.plot(epochs, valid_loss, c='green', linestyle='solid', label = 'valid loss')\n",
    "    plt.plot(epochs, test_loss, c='orange', linestyle='solid', label = 'test loss')\n",
    "    plt.xlabel('number of epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    plt.title(\"classification errors\")\n",
    "    plt.plot(epochs, training_err, c='blue', linestyle='dashed', label = 'train error')\n",
    "    plt.plot(epochs, valid_err, c='green', linestyle='dashed', label = 'valid error')\n",
    "    plt.plot(epochs, test_err, c='orange', linestyle='dashed', label = 'test error')\n",
    "    plt.xlabel('number of epoch')\n",
    "    plt.ylabel('error')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 10\n",
    "\n",
    "> Train your network on the Fashion MNIST dataset. Plot the training/valid/test curves (error and loss as a function of the epoch number, corresponding to what you wrote in a file in the last question). Add to your report the curves obtained using your best hyperparameters, i.e. for which you obtained your best error on the validations et. We suggest 2 plots: the first one will plot the error rate (train/valid/test with different colors, show which color in a legend) and the other one for the averaged loss (on train/valid/test). You should be able to get less than 20% test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves('errors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
